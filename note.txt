
音视频相关

视频文件组成方式：
	图像+音频
	图像经过视频编码压缩格式处理
	音频经过音频编码压缩格式处理
	相应的元信息（相当于音视频数据的注释）
	最后经过一次容器封装

视频播放流程
	数据->解协议（流媒体协议，如：RTMP，MMS）->解封装（视频封装格式如：MP4，MKV，RMVB，TS，FLV，AVI）->解码（音频压缩编码标准如：AAC，MP3，AC-3；视频压缩编码格式标准如：H.264，MPEG2，VC-1）


采样：把连续的时间信号，变成离散的数字信号
采样率：采样速度或采样频率，定义了每秒从连续信号中提取并组成离散信号的采样个数，用赫兹表示，采样率的倒数为采样周期（采样时间，采样之间的时间间隔）。通俗的讲采样率就是指计算机每秒采集多少个信号样本

PPI：
	Pixels Per Inch，每英寸像素数
FPS：
	Frame per second，视频每秒钟播放的画面数量


假设一个以颜色格式为YUV420来存储的视频文件，分辨率是1280x720，帧率是25fps，那么1分钟的数据量（字节数）是多少？

RGB：width * height * 3
RGBA8888：width * height * 4
YUV444：width * height * 3
YUV422：width * height * 2
YUV420：width * height * 3 / 2


FLV封装格式

结构




FLV头结构


FLV Header之后就是FLV File Body，由一连串back-pointers + tags构成。back-pointer表示Previous Tag Size，占4字节

FLV Tag结构


FLV Tag类型可以是音频、视频、Script（脚本类型）

Script Tag Data结构
	该类型又被称为MetaData Tag，存放一些关于FLV视频和音频的原信息。如：duration、width、height灯。通常该类型Tag会作为FLV文件的第一个Tag，且只有一个，跟在Header后，该类型结构

第一个AMF包：
	第一个字节表示AMF包类型，一般总是0x02，表示字符串。第2-3个字节为uint16_t类型，标识字符串长度，一般为0x000A（“onMetaData”长度）。后面字节为具体字符串，一般为“onMetaData”（6F,6E,4D,65,74,61,44,61,74,61）
第二个AMF包：
	第一个字节表示AMF包类型，一般为0x08，表示数组。第2-5个字节为uint32_t类型，表示数组元素个数，后面为各数组元素的封装，数组元素为元素名称和值组成的对

Audio Tag Data结构
	第一个字节包含音频数据的参数信息，从第二个字节开始为音频流数据





第二个字节为音频数据


Video Tag Data结构
第一个字节包含视频数据参数信息


第二个字节为视频数据


AAC编码格式
AAC文件格式有ADIF和ADTS两种
	ADIF（Audio Data Interchange Format，音频数据交换格式），特征是解码必须从明确定义的开始处进行，不能从数据流中间开始，只有一个统一的头
	ADTS（Audio Data Transport Stream，音频数据传输流），特征是有同步字，解码可以从流的任意位置开始，每一帧都有头信息，即ADTS_header，一般ADTS头信息都是7字节，若有CRC则为9字节
	ADTS帧首部结构



字段解析
Profile：决定了每个aac帧含有多少sample，对应关系如下

LC：Low Complexity 			HE：High Efficiency

Sampleing Frequency Index：使用的采样率下标，通过这个下标在SamplingFrequencies[]数组中查找得知采样率的值，对应关系如下
0			96000Hz				9			12000Hz
1			88200Hz				10			11025Hz
2			64000Hz				11			8000Hz
3			48000Hz				12			7350Hz
4			44100Hz					13			Reserved
5			32000Hz				14			Reserved
6			24000Hz				15			frequency is written explicitly
7			22050Hz
8			16000Hz

Channel Configuration：声道数，对应关系如下
0: Defined in AOT Specifc Config
1: 1 channel: front-center
2: 2 channels: front-left, front-right
3: 3 channels: front-center, front-left, front-right
4: 4 channels: front-center, front-left, front-right, back-center
5: 5 channels: front-center, front-left, front-right, back-left, back-right
6: 6 channels: front-center, front-left, front-right, back-left, back-right, LFE-channel
7: 8 channels: front-center, front-left, front-right, side-left, side-right, back-left, back-right, LFE-channel
8-15: Reserved


H.264编码格式
	理论依据：一段时间内图像的统计结果表明，在相邻几幅图像画面中，一般有差别的像素只有10%以内的点，亮度差值变化不超过2%，而色度差值的变化只有1%以内。
	h.264原始码流（裸流），是由一个个NALU组成。它的功能分为两层：视频编码层（VCL，Video Coding Layer）和网络提取层（NAL，Network Abstration Layer）
	VCL数据即编码处理的输出，表示被压缩编码后的视频数据序列。在VCL数据传输或存储之前，这些数据会先被映射或封装进NAL中
	VCL：视频编码层（Video Coding Layer）	
		1、压缩：预测（帧内预测、帧间预测）->DCT变化、量化->编码
		2、切分数据：切分成条带、宏块等。提高编码效率和降低误码率；提高网络传输的灵活性
		3、包装成NAL
	NAL：网络抽象层（Network Abstraction Layer），将VCL的数据打包
	NALU格式
		每个 NALU 包括一个原始字节序列负荷(RBSP, Raw Byte Sequence Payload)、一组 对应于视频编码的 NALU 头部信息。RBSP 的基本结构是:在原始编码数据的后面填加了结尾 比特。一个 bit“1”若干比特“0”，以便字节对齐。
		NAL头 ｜ RBSP ｜ NAL头 ｜ RBSP
		NALU Header
		｜ forbidden_zero_bit ｜ nal_ref_idc ｜ nal_unit_type｜
				1bit				2 bit				5bit
		forbidden_zero_bit：禁止位。在网络传输中发生错误时，会被置为1，告知接收方丢掉该单元；否则为0
		nal_ref_idc：指示当前NALU的优先级，值越大，优先级越高
		nal_unit_type：NALU类型

	EBSP、RBSP、SODB
		EBSP，扩展字节序列载荷（Excapsulated Byte Sequence Payload）
		RBSP，原始字节序列载荷（Raw Byte Sequence Payload）
		SODB，原始编码数据（String Of Data Bits）
		EBSP包含RBSP，RBSP包含SODB
	
	严格来说
		NALU = NALU Header + EBSP（h264文档中没有EBSP这一概念，但是h264官方参考软件JM中使用了EBSP）
		

		EBSP相较于RBSP，对了一个防止竞争的字节：0x03。NALU起始码为0x000001或0x00000001，而h264规定，当监测到0x000000时，也可以表示当前NALU的结束。为了防止在NALU内部出现起始码，所以h264提出“防止竞争”机制，当编码器编码完一个NAL时，应监测NALU内部是否出现0x00000x（0x000001、0x000002、0x000003），当监测到它们存在时，编码器会在最后一个字节前，插入一个新的字节：0x03。则当拿到EBSP时，就需要监测EBSP内是否有序列：0x000003，若有，则去掉0x03就能得到RBSP

		RBSP = SODB + RBSP尾部
		RBSP尾部：
			1、RBSP尾部（大多数类型的NALU使用这种尾部）
				rbsp_stop_one_bit ｜ rbsp_alignment_zero_bit
						1bit（1）				x bits（0）
			2、条带RBSP尾部（当NALU类型为条带，即nal_unit_type为1～5时，RBSP使用该尾部）
			
		当拿到RBSP时，去掉尾部就能得到SODB
		综上所述，EBSP去掉0x03—>RBSP去掉尾部—>SODB
				


特殊NALU类型：SPS、PPS
	SPS：序列参数集，Sequence Parameter Set。保存一组编码视频序列的全局参数
	PPS：图像参数集，Picture Parameter Set，保存整体图像相关的参数

AU分隔符
	Access Unit Delimiter，在解码值中识别帧边界











片（slice）的概念不同于帧（frame），帧用作描述一张图片，一帧对应一张图片，片是h.264中提出的新概念，一张图片至少有一个或多个片

片都是由NALU装载并进行网络传输的，并不代表NALU内部数据一定是片，因为NALU还有可能装载其他描述视频的信息

片类型
	I片：只包含I宏块，I宏块利用从当前片中已解码的像素作为参考进行帧内预测（不嫩取其他片中的已解码像素来参考）
	P片：可包含P和I宏块。P宏块利用前面已编码图像作为参考图像进行帧内预测，一个帧内编码的宏块可进一步做宏块的分割，即：16x16、16x8、8x16、8x8亮度像素块(以及附带的彩色像素);如果选了 8×8 的子宏块，则可再分成各种子宏块的分割，其尺寸为 8×8、8×4、4×8 或 4×4 亮度像素块(以及附带的彩色像素)。
	B片：可包含B和I宏块。B宏块利用双向的参考图像
	SP片：用于不同编码流之间的切换，包含P 和/或 I宏块
	SI片：扩展档次中必须具有的切换

GOP（图像组，Group Of Picture）主要形容一个I帧到下一个I帧之间的间隔帧数，增大图片组能有效减少编码后的视频体积，但是也会降低视频质量



ACC音频文件格式：
	ADIF：Audio Data Interchange Format，音频数据交换格式。可以确定的找到音频数据的开始，不需要在数据流中间进行解码，即解码必须在明确定义的位置开始。该格式的文件通常存储在磁盘中
	格式：header（）｜ raw_data_stream（）
	ADTS：Audio Data Transport Stream，音频数据传输流。是一个有同步字的传输流，解码可以从任意位置开始。该格式特征类似于mp3，可应用于广播电视
	格式：	｜syncword｜header()｜error_check（）｜raw_data_block（）｜	



RTP协议
RTP（real-time transport protocol，实时传输协议），为实时传输交互的音视频提供了端到端传输服务。其中包括载荷的类型确认，序列编码，时间戳和传输监控功能。一般基于UDP协议，使用RTP的多路技术以及验和服务。也可以与其他适合的协议并用，若底层网络支持多路分发，RTP也可将数据传输给多个目标。但RTP不提供任何机制保证数据的实时性和QOS（quality-of-service），而是依赖底层的服务，RTP既不保证传输的可靠性也不保证无序传输，也不假定底层网络是可信任和有序的


SSRC（Synchronization Source）：RTP报文流中的一个source，由RTP头中定义的32bits的SSRC identifier标识，这样做是为了不依赖网络地址。同一个SSRC中发送的所有包都具有同一时序和序列号间隔，因此接收者可以通过SSRC将收到的数据包分组并排序。一个信号源（麦克风，摄像头，Mixer）的报文流会有由一个SSRC的发送器发送。一个SSRC可能会随着时间的变化，改变其数据格式，例如音频编码。SSRC的身份识别码都是随机生成的，但是必须保证整个RTP session中该身份识别码不会重复，这些工作是通过RTCP来完成的。如果一个与会者在一个RTP session中发送不同的媒体数据流，那么每个流的SSRC必须不同。
Contributing source (CSRC)：RTP Mixer所混合的所有数据对应的SSRC的列表。Mixer会将一个SSRC列表写入RTP头中，该列表包含了这个混合报文中包含的所有来源SSRC。
End system：一个生成RTP payload和消费收到的RTP payload的应用。一个End system可以扮演一个或者多个SSRC角色，但是通常是一个。
Mixer：一个中介系统，它接受一个或多个Source的数据，随后它可能会改变这些数据的格式，并将它们合并为一个新的RTP packet。因为，多个输入源的时序通常来说都不一致，所以Mixer通常会同步不同源的时间，并生成一个自己的时序来处理合并数据流。所有从Mixer输出的数据包都会标记上该Mixer的SSRC。
Translator：一个中介系统，它会转发RTP packet但是不改变其原本的SSRC。
Monitor：一个在RTP session中接收RTCP报文的应用，它会总结数据被接受的报告，并为当前分发系统评估QOS，诊断错误，长期统计。Monitor可以集成进会议应用中，也可以是独立的第三方应用，只接受RTCP报文，但是什么都不发送。
Non-RTP means：为了让RTP提供可用服务而加入的协议或者机制。特别是在多媒体会议中，需要一种控制协议来分发组播地址和加密密钥，协调加密算法，定义RTP payload格式和RTP payload类型的动态映射。



包头（前12个字节出现在每个RTP包中，仅在被混合器插入时，才有CSRC）

V（版本）：2bits
P（填充）：1bit，若被设置，则此包包含一到多个附加在末端的填充比特，填充比特不算负载的一部分。填充的最后一个字节指明可忽略多少填充比特
X（扩展）：1bit，若被设置，则固定包头后面跟随一个头扩展
CC（CSRC计数）：4bits，包含了跟在固定头后面CSRC标识符的数量
M（标志）：1bit，由具体协议规定，用来允许在码流中标记重要的事件，如帧边界
PT（负载类型）：7bits
Sequence Number（序列号）：16bits，每发送一个RTP包，序列号+1，接收端可据此监测丢包和重建包序列。初始值随机
TimeStamp（时间戳）：32bits，反映TRP包中第一个字节的采样时间，时钟频率依赖于负载数据格式，并在描述文件中进行描述，也可用RTP方法对负载格式动态描述
SSRC（同步信源）：32bits，用以识别同步源，保证在同一个RTP会话中没有任何两个同步源有相同的SSRC识别符，这样做是为了不依赖网络地址。同一个SSRC中发送的所有包都具有同一时序和序列号间隔，接收者可以根据SSRC将收到的数据包分组并排序
同步源：来自同一信号源的包流的发送方，如麦克风、摄像头、RTP混频器就是同步源。一个同步源可能随着时间变化而改变其数据格式，如音频编码。SSRC标识符是随机选取的值，在特定的RTP会话中是全局唯一的（globally unique），该工作由RTCP完成。参与者不用在一个多媒体会议的所有RTP会话中使用相同的SSRC标识符；SSRC标识符的绑定通过RTCP。若参与者在一个RTP会话中生成了多个流，如来自多个摄像头，则每个摄像头都必须标识成单独的同步源
CSRC （特约信源）：0 到 15 项，每项 32 比特，CSRC 列表识别在此包中负载的所有贡献源。识别符的数目在 CC 域中给定。若有贡献源多于 15 个，仅识别 15 个。CSRC 识别符由混合器插入，并列出所有贡献源的 SSRC 识别符。例如语音包，混合产生新包的所有源的 SSRC 标识符都被列出，以在接收端处正确指示参与者。

扩展头结构

若 RTP 固定头中的扩展比特位置 1，则一个长度可变的头扩展部分被加到 RTP 固定头之后。头扩展包含 16 比特的长度域，指示扩展项中 32 比特字的个数，不包括 4 个字节扩展头(因此零是有效值)。RTP 固定头之后只允许有一个头扩展。为允许多个互操作实现独立生成不同的头扩展，或某种特定实现有多种不同的头扩展,扩展项的前 16 比特用以识别标识符或参数。这 16 比特的格式由具体实现的上层协议定义。基本的 RTP 说明并不定义任何头扩展本身。


编码层次组成：
	序列（Sequence）
	图像组（Group of Pictures， GOP）
	图像（Picture）
	条带（Slice）
	宏块（Macroblock， MB）
	块（Block）


图像编码结构：
	图像
	图像起始码：专有的一段比特串，标识一个图像的压缩数据的开始，如MPEG-2的图像起始码为0x000001
	图像头：记录图像信息（包含图像编码类型，图像距离，图像编码结构，图像是否为逐行扫描）

条带编码结构：
	条带：多个宏块的组合
	条带起时码：专有的一段比特串，标识一个条带的压缩数据的开始。如MPEG-2的条带起始码为十六进制数000001(0~AF)。
	条带头：记录当前图像的相关信息。含条带位置，条带量化参数，宏块编码技术标识等

宏块编码结构：
	宏块：16X16的像素块
	宏块内容：宏块编码类型，编码模式，参考帧索引，运动矢量信息，宏块编码系数

块编码结构：
	8X8或4X4的变换量化系数的熵编码数据
	CBP（Coded Block Patten）：用来指示块的变换量化系数是否全为0
		对于YUV（4:2:0）编码，CBP通常6bit长，每个bit对应一个块，当某一块的变换两化系数全为0时，其对应bit位值为0，否则为1
	每个块的变换量化系数的最后用一个EOB（End of Block）符号来标识

视频编解码关键技术：
	预测：通过帧内预测和帧间预测降低视频图像的空间冗余和时间冗余
	变换：通过从时域到频域的变换，去除相邻数据之间的相关性，即去除空间冗余
	量化：通过用更粗糙的数据表示更精细的数据来降低编码的数据量，或者通过去除人眼不敏感的信息来降低编码数据量
	扫描：将二位变换量化数据重新组织称一维的数据序列
	熵编码：根据待编码数据的概率特性减少编码冗余


预测：
	空间预测：利用图像空间相邻像素的相关性来预测的方法
		帧内预测：利用当前编码块周围已经重构出来的像素预测当前块
		Intra图像编码（I帧）
	时间预测：利用时间上相邻图像的相关性来预测的方法
		帧间预测：运动估计（Motion Estimation，ME），运动补偿（Motion Compensation，MC）
		Inter图像编码：前向预测编码图像（P帧），双向预测编码图像（B帧）

运动估计：一般将当前的输入图像分割成若干彼此不相重叠的小图像子块，例如一帧图像大小为1280*720，首先将其以网格状的形式分成40*45个尺寸为16*16的彼此没有重叠的图像块，然后在前一图像块或后 一图像某个搜索窗口的范围内为每一个图像块寻找一个与之最为相似的图像块，这个搜索的过程叫做运动估计。
通过计算最相似的图像块与该图像块之间的位置信息，可以得到一个运动矢量。这样在编码过程中就可以将当前图像中的块与参考图像运动矢量所指向的最相似的图像块相减，得到一个残差图像块，由于残差图像块中的每个像素值很小，所以在压缩编码中可以获得更高的压缩比，这个相减过程叫做运动补偿


帧内预测：
	I帧图像的每个宏块都采用帧内（Intra）预测编码模式
	宏块分成8X8或4X4块，对每个块采用帧内预测编码，称作Intra8X8或Intra4X4
	帧内预测有多个预测方向：水平，垂直，左下，右上
	帧内预测还有直流（DC）预测
	色度块预测还有平面预测


量化：
	原理：将含有大量的数据集合映射到含有少量的数据集合中

码率控制：
	收到缓冲区，带宽的限制，编码码率不能无限制的增长，因此需要通过控制码率来将编码码流控制在目标码率范围内
	一般通过调整量化参数的手段控制码率：
		帧级控制
		条带级控制
		宏块级控制
	码率控制考虑的问题：
		防止码流有较大的波动，导致缓冲区溢出
		同时保持缓冲区尽可能充满，让图像质量尽可能的好且稳定
	CBR（Constant Bit Rate）：比特率稳定，但图像质量变化大
	VBR（Variable Bit Rate）：比特率波动大，但图像质量稳定
	码率控制算法：
		码率分配、码率控制
	码率控制属于非标准技术，编码端有，解码端没有


混合编码：
	当前输入的图像首先经过分块，分块得到的图像块要与经过运动补偿的预测图像相减得到差值图像X，然后对该差值图像块进行DCT变换和量化，量化输出的数据有两个不同的去处：一个是送给熵编码器进行编码，编码后的码流输出到一个缓存器中保存，等待传送出去。另一个应用是进行反量化和反变化后的到信号X‘，该信号将与运动补偿输出的图像块相加得到新的预测图像信号，并将新的预测图像块送至帧存储器




量化和量化器：
	量化和量化器：量化是把离散时间上的连续信号，转化成离散时间上的离散信号
	常见量化器：均匀量化器，对数量化器，非均匀量化器
	量化过程追求的目标：最小化量化误差，并尽量减低量化器的复杂度

常见量化器的优缺点：
	均匀量化器：最简单，性能最差，仅适用于电话语音
	对数量化器：比均匀量化器复杂，也容易实现，性能比均匀量化器好
	非均匀（Non-uniform）量化器：根据信号的分布情况，来设计量化器。信号密集的地方进行细致的量化，稀疏的地方进行粗略量化



频谱掩蔽效应：
	一个频率的声音能量小于某个阈值之后，人耳就会听不到，这个阈值称为最小可闻阈。当有另外能量较大的声音出现的时候，该声音频率附近的阈值会提高很多，即所谓的掩蔽效应


时域掩蔽效应：
	当强音信号和弱音信号同时出现时，还存在时域掩蔽效应。即两者发生时间很近的时候，也会发生掩蔽效应。时域掩蔽效应分为三种：前掩蔽，同时掩蔽，后掩蔽。前掩蔽是指人耳在听到强信号之前的短暂时间内，已经存在的弱信号会被掩蔽而听不到。同时掩蔽是指当强信号与弱信号同时存在时，弱信号会被强信号所掩蔽而听不到。后掩蔽是指当强信号消失后，需经过较长时间才能重新听见弱信号，称为后掩蔽。这些被掩蔽的弱信号即可视为冗余信号。

音频压缩编码方式：
	对每一个音频声道中的音频采样信号，首先都要将他们映射到频域中，这种时域到频域的映射可通过子带滤波器实现。每个升到中的音频采样块首先要根据心理声学模型来计算掩蔽门限值，然后由计算出的掩蔽门限值决定从公共比特池中分配给该声道的不同频率域中多少比特数，接着进行量化以及编码，最后将控制参数及辅助数据加入数据之中，产生编码后的数据流
	通过子带滤波器将每个音频声道中的音频采样信号由时域映射到频域中
	根据心理声学模型计算每个声道中的音频采样块的掩蔽门限值
	有掩蔽门限值决定从公共比特池中分配给该省到的不同频域中多少比特
	量化、编码
	将控制参数及辅助数据加入数据中



音视频同步方法：
	1、将视频和音频同步到外部时钟上
	2、将音频同步到视频上
	3、将视频同步到音频上

PTS：Presentation Time Stamp，显示时间戳。类型uint64_t，单位为time_base，1/90000秒
DTS：Decode Time Stamp，解码时间戳
CTS：Composition Time，在avc视频帧里没有存储pts，而是通过dts和cts来计算，cts = （pts - dts） / 90
注：虽然DTS、PTS是用于知道播放短的行为，但他们是在编码时由编码器生成的
时间基：time_base， 以秒为单位，类型为AVRational，假设时间基为a，则时间基表示每隔a秒显示一帧数据
	typedef struct AVRational
	{
		int num;	//分子
		int den;	//分母
	} AVRational;
	

av_q2d()：将一个AVRational转换为双精度浮点数


解码顺序与播放顺序：
	由于GOP中I、B、P帧的存在，所以视频数据的解码顺序和播放顺序不相同，而音频数据的解码顺序和播放顺序一致
	I帧是关键帧，包含一帧完整数据，可以直接显示，不需要参考其他帧
	P帧前向预测帧，只需参考前一帧的数据即可解码显示，要显示P帧则需要缓存其前一帧的数据
	B帧双向预测帧，需要参考前后两帧的数据才能解码显示，要显示B帧需要得到其前一帧和后一帧的数据
	例：
		Stream：I P B B
		DTS：	1 2 3 4 
		PTS：	1 4 2 3 


Audio_Clock：Audio的播放时长，从开始到当前的时间。
	1、由于一个packet中可包含多个Frame，packet中的PTS比真正播放的PSTS早很多，可根据Sample Rate和Sample Format来计算该packet中的数据可播放的时长，再次更新Audio_Clock
	2、在获取这个Audio_Clock时，可能音频缓冲区还有未播放结束的数据，即有一部分数据还未播放，故需要用Audio_Clock减去这部分时间，得到结果即为真正Audio_Clock


Video_Clock：视频播放到当前帧时已播放的时间长度

视频流时间长度计算方法(st为一个stream的指针)：
	time(second ) = st->duration * av_a2d(st->time_base)

视频中某一帧的显示时间：
	timestamp(second) = its * av_q2d(st->time_base)

音视频同步：
	1、当前帧的PTS - 上一帧的PTS得到延迟时间
	2、当前帧的PTS和Audio_Clock比较，判断视频播放速度快慢
	3、根据第二步的结果，设置播放下一帧的延迟时间


视频同步到音频基本方法：
	若视频超前音频，则不播放，以等待音频；若视频落后音频，则 丢弃当前帧直接播放下一帧，以追赶音频
	展示第一帧视频后，获得要显示的下一个视频帧的PTS，然后设置一个定时器，当定时器超时后，刷新新的视频帧如此反复操作


音频码率计算公式
	采样率值×采样大小值×声道数bps

音频文件大小
	时长 x  码率 / 1024 / 8 （MB）

每帧PCM数据大小：
	采样率*采样时间*采样位深/8*通道数（Bytes）

AAC: nb_samples和frame_size = 1024
一帧数据量：1024*2*s16/8 = 4096个字节。
ACC帧率 (一秒播放帧数)= TotalByte/4096 = 43.06640625帧

MP3: nb_samples和frame_size = 1152
一帧数据量：1152*2*s16/8 = 4608个字节。
MP3帧率 (一秒播放帧数)= TotalByte/4608 = 38.28125帧


AEC
回波抵消（AEC）指的是在二线传输的两个方向上同时间、同频谱地占用线路，在线路上两个方向传输的信号完全混在一起，本端发信号的回波即成为本端收信号的干扰信号，利用自适应滤波器可抵消回波以达到较好的接收信号质量。也就是消除回声。
回声消除的原理就是利用接收到的音频与本地采集的音频做对比，添加反相的人造回声，将远端的声音消除。




ANS
背景噪声抑制（ANS）指的是将声音中的背景噪声识别并进行消除的处理。
背景噪声分为平衡噪声和瞬时噪声两类，平稳噪声的频谱稳定，瞬时噪声的频谱能量方差小，利用噪声的特点，对音频数据添加反向波形处理，即可消除噪声。



AGC
自动增益控制（AGC）是指当直放站工作于最大增益且输出为最大功率时，增加输入信号电平，提高直放站对输出信号电平控制的能力。自动增益控制主要用于调整音量幅值。
正常人交谈的音量在40~60dB之间，低于25dB的声音听起来很吃力，超过100dB的声音会让人不适。AGC的作用就是将音量调整到人接受的范围。
AGC的调整分为模拟部分和数字部分，模拟部分是麦克风的采集增益，数字部分是音频数据的数字电平调整。


VAD
	Voice Activity Detection，语音端点监测，一般用于鉴别音频信号中的语音出现和语音消失



硬解码：
	由CPU核心GPU对高清视频进行解码，CPU占用率很低，画质效果比软解码略差，需要对播放器进行设置
	优点：播放流畅，低功耗
	缺点：受视频格式限制，功耗大，画质没有软解码好
软解码：
	由CPU负责解码进行播放
	优点：不受视频格式限制，画质略好于硬解码
	缺点：占用过高资源，对于高清视频可能没有硬解码流畅


SEI
	补充增强信息（Supplemental Enhancement Information），属于码流范畴，提供向视频码流中加入额外信息的方法，是H.264/H.265等视频压缩标准的特性之一
	特征：
		1、并非解码过程的必选项
		2、可能对解码过程（容错，纠错）有帮助
		3、集成在视频码流中
	信息：
		1、编码器参数
		2、视频版权信息
		3、摄像头参数
		4、内容生成过程中的剪辑事件（引发场景切换）
音视频相关

视频文件组成方式：
	图像+音频
	图像经过视频编码压缩格式处理
	音频经过音频编码压缩格式处理
	相应的元信息（相当于音视频数据的注释）
	最后经过一次容器封装

视频播放流程
	数据->解协议（流媒体协议，如：RTMP，MMS）->解封装（视频封装格式如：MP4，MKV，RMVB，TS，FLV，AVI）->解码（音频压缩编码标准如：AAC，MP3，AC-3；视频压缩编码格式标准如：H.264，MPEG2，VC-1）


采样：把连续的时间信号，变成离散的数字信号
采样率：采样速度或采样频率，定义了每秒从连续信号中提取并组成离散信号的采样个数，用赫兹表示，采样率的倒数为采样周期（采样时间，采样之间的时间间隔）。通俗的讲采样率就是指计算机每秒采集多少个信号样本

PPI：
	Pixels Per Inch，每英寸像素数
FPS：
	Frame per second，视频每秒钟播放的画面数量


假设一个以颜色格式为YUV420来存储的视频文件，分辨率是1280x720，帧率是25fps，那么1分钟的数据量（字节数）是多少？

RGB：width * height * 3
RGBA8888：width * height * 4
YUV444：width * height * 3
YUV422：width * height * 2
YUV420：width * height * 3 / 2


FLV封装格式

结构
￼

￼

FLV头结构
￼

FLV Header之后就是FLV File Body，由一连串back-pointers + tags构成。back-pointer表示Previous Tag Size，占4字节

FLV Tag结构
￼

FLV Tag类型可以是音频、视频、Script（脚本类型）

Script Tag Data结构
	该类型又被称为MetaData Tag，存放一些关于FLV视频和音频的原信息。如：duration、width、height灯。通常该类型Tag会作为FLV文件的第一个Tag，且只有一个，跟在Header后，该类型结构
￼
第一个AMF包：
	第一个字节表示AMF包类型，一般总是0x02，表示字符串。第2-3个字节为uint16_t类型，标识字符串长度，一般为0x000A（“onMetaData”长度）。后面字节为具体字符串，一般为“onMetaData”（6F,6E,4D,65,74,61,44,61,74,61）
第二个AMF包：
	第一个字节表示AMF包类型，一般为0x08，表示数组。第2-5个字节为uint32_t类型，表示数组元素个数，后面为各数组元素的封装，数组元素为元素名称和值组成的对

Audio Tag Data结构
	第一个字节包含音频数据的参数信息，从第二个字节开始为音频流数据
￼

￼
￼

第二个字节为音频数据
￼

Video Tag Data结构
第一个字节包含视频数据参数信息
￼

第二个字节为视频数据


AAC编码格式
AAC文件格式有ADIF和ADTS两种
	ADIF（Audio Data Interchange Format，音频数据交换格式），特征是解码必须从明确定义的开始处进行，不能从数据流中间开始，只有一个统一的头
	ADTS（Audio Data Transport Stream，音频数据传输流），特征是有同步字，解码可以从流的任意位置开始，每一帧都有头信息，即ADTS_header，一般ADTS头信息都是7字节，若有CRC则为9字节
	ADTS帧首部结构
￼
￼

字段解析
Profile：决定了每个aac帧含有多少sample，对应关系如下
￼
LC：Low Complexity 			HE：High Efficiency

Sampleing Frequency Index：使用的采样率下标，通过这个下标在SamplingFrequencies[]数组中查找得知采样率的值，对应关系如下
0			96000Hz				9			12000Hz
1			88200Hz				10			11025Hz
2			64000Hz				11			8000Hz
3			48000Hz				12			7350Hz
4			44100Hz					13			Reserved
5			32000Hz				14			Reserved
6			24000Hz				15			frequency is written explicitly
7			22050Hz
8			16000Hz

Channel Configuration：声道数，对应关系如下
0: Defined in AOT Specifc Config
1: 1 channel: front-center
2: 2 channels: front-left, front-right
3: 3 channels: front-center, front-left, front-right
4: 4 channels: front-center, front-left, front-right, back-center
5: 5 channels: front-center, front-left, front-right, back-left, back-right
6: 6 channels: front-center, front-left, front-right, back-left, back-right, LFE-channel
7: 8 channels: front-center, front-left, front-right, side-left, side-right, back-left, back-right, LFE-channel
8-15: Reserved


H.264编码格式
	理论依据：一段时间内图像的统计结果表明，在相邻几幅图像画面中，一般有差别的像素只有10%以内的点，亮度差值变化不超过2%，而色度差值的变化只有1%以内。
	h.264原始码流（裸流），是由一个个NALU组成。它的功能分为两层：视频编码层（VCL，Video Coding Layer）和网络提取层（NAL，Network Abstration Layer）
	VCL数据即编码处理的输出，表示被压缩编码后的视频数据序列。在VCL数据传输或存储之前，这些数据会先被映射或封装进NAL中
	VCL：视频编码层（Video Coding Layer）	
		1、压缩：预测（帧内预测、帧间预测）->DCT变化、量化->编码
		2、切分数据：切分成条带、宏块等。提高编码效率和降低误码率；提高网络传输的灵活性
		3、包装成NAL
	NAL：网络抽象层（Network Abstraction Layer），将VCL的数据打包
	NALU格式
		每个 NALU 包括一个原始字节序列负荷(RBSP, Raw Byte Sequence Payload)、一组 对应于视频编码的 NALU 头部信息。RBSP 的基本结构是:在原始编码数据的后面填加了结尾 比特。一个 bit“1”若干比特“0”，以便字节对齐。
		NAL头 ｜ RBSP ｜ NAL头 ｜ RBSP
		NALU Header
		｜ forbidden_zero_bit ｜ nal_ref_idc ｜ nal_unit_type｜
				1bit				2 bit				5bit
		forbidden_zero_bit：禁止位。在网络传输中发生错误时，会被置为1，告知接收方丢掉该单元；否则为0
		nal_ref_idc：指示当前NALU的优先级，值越大，优先级越高
		nal_unit_type：NALU类型

	EBSP、RBSP、SODB
		EBSP，扩展字节序列载荷（Excapsulated Byte Sequence Payload）
		RBSP，原始字节序列载荷（Raw Byte Sequence Payload）
		SODB，原始编码数据（String Of Data Bits）
		EBSP包含RBSP，RBSP包含SODB
	
	严格来说
		NALU = NALU Header + EBSP（h264文档中没有EBSP这一概念，但是h264官方参考软件JM中使用了EBSP）
		

		EBSP相较于RBSP，对了一个防止竞争的字节：0x03。NALU起始码为0x000001或0x00000001，而h264规定，当监测到0x000000时，也可以表示当前NALU的结束。为了防止在NALU内部出现起始码，所以h264提出“防止竞争”机制，当编码器编码完一个NAL时，应监测NALU内部是否出现0x00000x（0x000001、0x000002、0x000003），当监测到它们存在时，编码器会在最后一个字节前，插入一个新的字节：0x03。则当拿到EBSP时，就需要监测EBSP内是否有序列：0x000003，若有，则去掉0x03就能得到RBSP

		RBSP = SODB + RBSP尾部
		RBSP尾部：
			1、RBSP尾部（大多数类型的NALU使用这种尾部）
				rbsp_stop_one_bit ｜ rbsp_alignment_zero_bit
						1bit（1）				x bits（0）
			2、条带RBSP尾部（当NALU类型为条带，即nal_unit_type为1～5时，RBSP使用该尾部）
			
		当拿到RBSP时，去掉尾部就能得到SODB
		综上所述，EBSP去掉0x03—>RBSP去掉尾部—>SODB
				
￼

特殊NALU类型：SPS、PPS
	SPS：序列参数集，Sequence Parameter Set。保存一组编码视频序列的全局参数
	PPS：图像参数集，Picture Parameter Set，保存整体图像相关的参数

AU分隔符
	Access Unit Delimiter，在解码值中识别帧边界

￼

￼

￼

￼

￼

片（slice）的概念不同于帧（frame），帧用作描述一张图片，一帧对应一张图片，片是h.264中提出的新概念，一张图片至少有一个或多个片

片都是由NALU装载并进行网络传输的，并不代表NALU内部数据一定是片，因为NALU还有可能装载其他描述视频的信息

片类型
	I片：只包含I宏块，I宏块利用从当前片中已解码的像素作为参考进行帧内预测（不嫩取其他片中的已解码像素来参考）
	P片：可包含P和I宏块。P宏块利用前面已编码图像作为参考图像进行帧内预测，一个帧内编码的宏块可进一步做宏块的分割，即：16x16、16x8、8x16、8x8亮度像素块(以及附带的彩色像素);如果选了 8×8 的子宏块，则可再分成各种子宏块的分割，其尺寸为 8×8、8×4、4×8 或 4×4 亮度像素块(以及附带的彩色像素)。
	B片：可包含B和I宏块。B宏块利用双向的参考图像
	SP片：用于不同编码流之间的切换，包含P 和/或 I宏块
	SI片：扩展档次中必须具有的切换

GOP（图像组，Group Of Picture）主要形容一个I帧到下一个I帧之间的间隔帧数，增大图片组能有效减少编码后的视频体积，但是也会降低视频质量



ACC音频文件格式：
	ADIF：Audio Data Interchange Format，音频数据交换格式。可以确定的找到音频数据的开始，不需要在数据流中间进行解码，即解码必须在明确定义的位置开始。该格式的文件通常存储在磁盘中
	格式：header（）｜ raw_data_stream（）
	ADTS：Audio Data Transport Stream，音频数据传输流。是一个有同步字的传输流，解码可以从任意位置开始。该格式特征类似于mp3，可应用于广播电视
	格式：	｜syncword｜header()｜error_check（）｜raw_data_block（）｜	



RTP协议
RTP（real-time transport protocol，实时传输协议），为实时传输交互的音视频提供了端到端传输服务。其中包括载荷的类型确认，序列编码，时间戳和传输监控功能。一般基于UDP协议，使用RTP的多路技术以及验和服务。也可以与其他适合的协议并用，若底层网络支持多路分发，RTP也可将数据传输给多个目标。但RTP不提供任何机制保证数据的实时性和QOS（quality-of-service），而是依赖底层的服务，RTP既不保证传输的可靠性也不保证无序传输，也不假定底层网络是可信任和有序的


SSRC（Synchronization Source）：RTP报文流中的一个source，由RTP头中定义的32bits的SSRC identifier标识，这样做是为了不依赖网络地址。同一个SSRC中发送的所有包都具有同一时序和序列号间隔，因此接收者可以通过SSRC将收到的数据包分组并排序。一个信号源（麦克风，摄像头，Mixer）的报文流会有由一个SSRC的发送器发送。一个SSRC可能会随着时间的变化，改变其数据格式，例如音频编码。SSRC的身份识别码都是随机生成的，但是必须保证整个RTP session中该身份识别码不会重复，这些工作是通过RTCP来完成的。如果一个与会者在一个RTP session中发送不同的媒体数据流，那么每个流的SSRC必须不同。
Contributing source (CSRC)：RTP Mixer所混合的所有数据对应的SSRC的列表。Mixer会将一个SSRC列表写入RTP头中，该列表包含了这个混合报文中包含的所有来源SSRC。
End system：一个生成RTP payload和消费收到的RTP payload的应用。一个End system可以扮演一个或者多个SSRC角色，但是通常是一个。
Mixer：一个中介系统，它接受一个或多个Source的数据，随后它可能会改变这些数据的格式，并将它们合并为一个新的RTP packet。因为，多个输入源的时序通常来说都不一致，所以Mixer通常会同步不同源的时间，并生成一个自己的时序来处理合并数据流。所有从Mixer输出的数据包都会标记上该Mixer的SSRC。
Translator：一个中介系统，它会转发RTP packet但是不改变其原本的SSRC。
Monitor：一个在RTP session中接收RTCP报文的应用，它会总结数据被接受的报告，并为当前分发系统评估QOS，诊断错误，长期统计。Monitor可以集成进会议应用中，也可以是独立的第三方应用，只接受RTCP报文，但是什么都不发送。
Non-RTP means：为了让RTP提供可用服务而加入的协议或者机制。特别是在多媒体会议中，需要一种控制协议来分发组播地址和加密密钥，协调加密算法，定义RTP payload格式和RTP payload类型的动态映射。



包头（前12个字节出现在每个RTP包中，仅在被混合器插入时，才有CSRC）
￼
V（版本）：2bits
P（填充）：1bit，若被设置，则此包包含一到多个附加在末端的填充比特，填充比特不算负载的一部分。填充的最后一个字节指明可忽略多少填充比特
X（扩展）：1bit，若被设置，则固定包头后面跟随一个头扩展
CC（CSRC计数）：4bits，包含了跟在固定头后面CSRC标识符的数量
M（标志）：1bit，由具体协议规定，用来允许在码流中标记重要的事件，如帧边界
PT（负载类型）：7bits
Sequence Number（序列号）：16bits，每发送一个RTP包，序列号+1，接收端可据此监测丢包和重建包序列。初始值随机
TimeStamp（时间戳）：32bits，反映TRP包中第一个字节的采样时间，时钟频率依赖于负载数据格式，并在描述文件中进行描述，也可用RTP方法对负载格式动态描述
SSRC（同步信源）：32bits，用以识别同步源，保证在同一个RTP会话中没有任何两个同步源有相同的SSRC识别符，这样做是为了不依赖网络地址。同一个SSRC中发送的所有包都具有同一时序和序列号间隔，接收者可以根据SSRC将收到的数据包分组并排序
同步源：来自同一信号源的包流的发送方，如麦克风、摄像头、RTP混频器就是同步源。一个同步源可能随着时间变化而改变其数据格式，如音频编码。SSRC标识符是随机选取的值，在特定的RTP会话中是全局唯一的（globally unique），该工作由RTCP完成。参与者不用在一个多媒体会议的所有RTP会话中使用相同的SSRC标识符；SSRC标识符的绑定通过RTCP。若参与者在一个RTP会话中生成了多个流，如来自多个摄像头，则每个摄像头都必须标识成单独的同步源
CSRC （特约信源）：0 到 15 项，每项 32 比特，CSRC 列表识别在此包中负载的所有贡献源。识别符的数目在 CC 域中给定。若有贡献源多于 15 个，仅识别 15 个。CSRC 识别符由混合器插入，并列出所有贡献源的 SSRC 识别符。例如语音包，混合产生新包的所有源的 SSRC 标识符都被列出，以在接收端处正确指示参与者。

扩展头结构
￼
若 RTP 固定头中的扩展比特位置 1，则一个长度可变的头扩展部分被加到 RTP 固定头之后。头扩展包含 16 比特的长度域，指示扩展项中 32 比特字的个数，不包括 4 个字节扩展头(因此零是有效值)。RTP 固定头之后只允许有一个头扩展。为允许多个互操作实现独立生成不同的头扩展，或某种特定实现有多种不同的头扩展,扩展项的前 16 比特用以识别标识符或参数。这 16 比特的格式由具体实现的上层协议定义。基本的 RTP 说明并不定义任何头扩展本身。


编码层次组成：
	序列（Sequence）
	图像组（Group of Pictures， GOP）
	图像（Picture）
	条带（Slice）
	宏块（Macroblock， MB）
	块（Block）


图像编码结构：
	图像
	图像起始码：专有的一段比特串，标识一个图像的压缩数据的开始，如MPEG-2的图像起始码为0x000001
	图像头：记录图像信息（包含图像编码类型，图像距离，图像编码结构，图像是否为逐行扫描）

条带编码结构：
	条带：多个宏块的组合
	条带起时码：专有的一段比特串，标识一个条带的压缩数据的开始。如MPEG-2的条带起始码为十六进制数000001(0~AF)。
	条带头：记录当前图像的相关信息。含条带位置，条带量化参数，宏块编码技术标识等

宏块编码结构：
	宏块：16X16的像素块
	宏块内容：宏块编码类型，编码模式，参考帧索引，运动矢量信息，宏块编码系数

块编码结构：
	8X8或4X4的变换量化系数的熵编码数据
	CBP（Coded Block Patten）：用来指示块的变换量化系数是否全为0
		对于YUV（4:2:0）编码，CBP通常6bit长，每个bit对应一个块，当某一块的变换两化系数全为0时，其对应bit位值为0，否则为1
	每个块的变换量化系数的最后用一个EOB（End of Block）符号来标识

视频编解码关键技术：
	预测：通过帧内预测和帧间预测降低视频图像的空间冗余和时间冗余
	变换：通过从时域到频域的变换，去除相邻数据之间的相关性，即去除空间冗余
	量化：通过用更粗糙的数据表示更精细的数据来降低编码的数据量，或者通过去除人眼不敏感的信息来降低编码数据量
	扫描：将二位变换量化数据重新组织称一维的数据序列
	熵编码：根据待编码数据的概率特性减少编码冗余


预测：
	空间预测：利用图像空间相邻像素的相关性来预测的方法
		帧内预测：利用当前编码块周围已经重构出来的像素预测当前块
		Intra图像编码（I帧）
	时间预测：利用时间上相邻图像的相关性来预测的方法
		帧间预测：运动估计（Motion Estimation，ME），运动补偿（Motion Compensation，MC）
		Inter图像编码：前向预测编码图像（P帧），双向预测编码图像（B帧）

运动估计：一般将当前的输入图像分割成若干彼此不相重叠的小图像子块，例如一帧图像大小为1280*720，首先将其以网格状的形式分成40*45个尺寸为16*16的彼此没有重叠的图像块，然后在前一图像块或后 一图像某个搜索窗口的范围内为每一个图像块寻找一个与之最为相似的图像块，这个搜索的过程叫做运动估计。
通过计算最相似的图像块与该图像块之间的位置信息，可以得到一个运动矢量。这样在编码过程中就可以将当前图像中的块与参考图像运动矢量所指向的最相似的图像块相减，得到一个残差图像块，由于残差图像块中的每个像素值很小，所以在压缩编码中可以获得更高的压缩比，这个相减过程叫做运动补偿


帧内预测：
	I帧图像的每个宏块都采用帧内（Intra）预测编码模式
	宏块分成8X8或4X4块，对每个块采用帧内预测编码，称作Intra8X8或Intra4X4
	帧内预测有多个预测方向：水平，垂直，左下，右上
	帧内预测还有直流（DC）预测
	色度块预测还有平面预测


量化：
	原理：将含有大量的数据集合映射到含有少量的数据集合中

码率控制：
	收到缓冲区，带宽的限制，编码码率不能无限制的增长，因此需要通过控制码率来将编码码流控制在目标码率范围内
	一般通过调整量化参数的手段控制码率：
		帧级控制
		条带级控制
		宏块级控制
	码率控制考虑的问题：
		防止码流有较大的波动，导致缓冲区溢出
		同时保持缓冲区尽可能充满，让图像质量尽可能的好且稳定
	CBR（Constant Bit Rate）：比特率稳定，但图像质量变化大
	VBR（Variable Bit Rate）：比特率波动大，但图像质量稳定
	码率控制算法：
		码率分配、码率控制
	码率控制属于非标准技术，编码端有，解码端没有


混合编码：
	当前输入的图像首先经过分块，分块得到的图像块要与经过运动补偿的预测图像相减得到差值图像X，然后对该差值图像块进行DCT变换和量化，量化输出的数据有两个不同的去处：一个是送给熵编码器进行编码，编码后的码流输出到一个缓存器中保存，等待传送出去。另一个应用是进行反量化和反变化后的到信号X‘，该信号将与运动补偿输出的图像块相加得到新的预测图像信号，并将新的预测图像块送至帧存储器




量化和量化器：
	量化和量化器：量化是把离散时间上的连续信号，转化成离散时间上的离散信号
	常见量化器：均匀量化器，对数量化器，非均匀量化器
	量化过程追求的目标：最小化量化误差，并尽量减低量化器的复杂度

常见量化器的优缺点：
	均匀量化器：最简单，性能最差，仅适用于电话语音
	对数量化器：比均匀量化器复杂，也容易实现，性能比均匀量化器好
	非均匀（Non-uniform）量化器：根据信号的分布情况，来设计量化器。信号密集的地方进行细致的量化，稀疏的地方进行粗略量化



频谱掩蔽效应：
	一个频率的声音能量小于某个阈值之后，人耳就会听不到，这个阈值称为最小可闻阈。当有另外能量较大的声音出现的时候，该声音频率附近的阈值会提高很多，即所谓的掩蔽效应


时域掩蔽效应：
	当强音信号和弱音信号同时出现时，还存在时域掩蔽效应。即两者发生时间很近的时候，也会发生掩蔽效应。时域掩蔽效应分为三种：前掩蔽，同时掩蔽，后掩蔽。前掩蔽是指人耳在听到强信号之前的短暂时间内，已经存在的弱信号会被掩蔽而听不到。同时掩蔽是指当强信号与弱信号同时存在时，弱信号会被强信号所掩蔽而听不到。后掩蔽是指当强信号消失后，需经过较长时间才能重新听见弱信号，称为后掩蔽。这些被掩蔽的弱信号即可视为冗余信号。

音频压缩编码方式：
	对每一个音频声道中的音频采样信号，首先都要将他们映射到频域中，这种时域到频域的映射可通过子带滤波器实现。每个升到中的音频采样块首先要根据心理声学模型来计算掩蔽门限值，然后由计算出的掩蔽门限值决定从公共比特池中分配给该声道的不同频率域中多少比特数，接着进行量化以及编码，最后将控制参数及辅助数据加入数据之中，产生编码后的数据流
	通过子带滤波器将每个音频声道中的音频采样信号由时域映射到频域中
	根据心理声学模型计算每个声道中的音频采样块的掩蔽门限值
	有掩蔽门限值决定从公共比特池中分配给该省到的不同频域中多少比特
	量化、编码
	将控制参数及辅助数据加入数据中



音视频同步方法：
	1、将视频和音频同步到外部时钟上
	2、将音频同步到视频上
	3、将视频同步到音频上

PTS：Presentation Time Stamp，显示时间戳。类型uint64_t，单位为time_base，1/90000秒
DTS：Decode Time Stamp，解码时间戳
CTS：Composition Time，在avc视频帧里没有存储pts，而是通过dts和cts来计算，cts = （pts - dts） / 90
注：虽然DTS、PTS是用于知道播放短的行为，但他们是在编码时由编码器生成的
时间基：time_base， 以秒为单位，类型为AVRational，假设时间基为a，则时间基表示每隔a秒显示一帧数据
	typedef struct AVRational
	{
		int num;	//分子
		int den;	//分母
	} AVRational;
	

av_q2d()：将一个AVRational转换为双精度浮点数


解码顺序与播放顺序：
	由于GOP中I、B、P帧的存在，所以视频数据的解码顺序和播放顺序不相同，而音频数据的解码顺序和播放顺序一致
	I帧是关键帧，包含一帧完整数据，可以直接显示，不需要参考其他帧
	P帧前向预测帧，只需参考前一帧的数据即可解码显示，要显示P帧则需要缓存其前一帧的数据
	B帧双向预测帧，需要参考前后两帧的数据才能解码显示，要显示B帧需要得到其前一帧和后一帧的数据
	例：
		Stream：I P B B
		DTS：	1 2 3 4 
		PTS：	1 4 2 3 


Audio_Clock：Audio的播放时长，从开始到当前的时间。
	1、由于一个packet中可包含多个Frame，packet中的PTS比真正播放的PSTS早很多，可根据Sample Rate和Sample Format来计算该packet中的数据可播放的时长，再次更新Audio_Clock
	2、在获取这个Audio_Clock时，可能音频缓冲区还有未播放结束的数据，即有一部分数据还未播放，故需要用Audio_Clock减去这部分时间，得到结果即为真正Audio_Clock


Video_Clock：视频播放到当前帧时已播放的时间长度

视频流时间长度计算方法(st为一个stream的指针)：
	time(second ) = st->duration * av_a2d(st->time_base)

视频中某一帧的显示时间：
	timestamp(second) = its * av_q2d(st->time_base)

音视频同步：
	1、当前帧的PTS - 上一帧的PTS得到延迟时间
	2、当前帧的PTS和Audio_Clock比较，判断视频播放速度快慢
	3、根据第二步的结果，设置播放下一帧的延迟时间


视频同步到音频基本方法：
	若视频超前音频，则不播放，以等待音频；若视频落后音频，则 丢弃当前帧直接播放下一帧，以追赶音频
	展示第一帧视频后，获得要显示的下一个视频帧的PTS，然后设置一个定时器，当定时器超时后，刷新新的视频帧如此反复操作


音频码率计算公式
	采样率值×采样大小值×声道数bps

音频文件大小
	时长 x  码率 / 1024 / 8 （MB）

每帧PCM数据大小：
	采样率*采样时间*采样位深/8*通道数（Bytes）

AAC: nb_samples和frame_size = 1024
一帧数据量：1024*2*s16/8 = 4096个字节。
ACC帧率 (一秒播放帧数)= TotalByte/4096 = 43.06640625帧

MP3: nb_samples和frame_size = 1152
一帧数据量：1152*2*s16/8 = 4608个字节。
MP3帧率 (一秒播放帧数)= TotalByte/4608 = 38.28125帧


AEC
回波抵消（AEC）指的是在二线传输的两个方向上同时间、同频谱地占用线路，在线路上两个方向传输的信号完全混在一起，本端发信号的回波即成为本端收信号的干扰信号，利用自适应滤波器可抵消回波以达到较好的接收信号质量。也就是消除回声。
回声消除的原理就是利用接收到的音频与本地采集的音频做对比，添加反相的人造回声，将远端的声音消除。




ANS
背景噪声抑制（ANS）指的是将声音中的背景噪声识别并进行消除的处理。
背景噪声分为平衡噪声和瞬时噪声两类，平稳噪声的频谱稳定，瞬时噪声的频谱能量方差小，利用噪声的特点，对音频数据添加反向波形处理，即可消除噪声。



AGC
自动增益控制（AGC）是指当直放站工作于最大增益且输出为最大功率时，增加输入信号电平，提高直放站对输出信号电平控制的能力。自动增益控制主要用于调整音量幅值。
正常人交谈的音量在40~60dB之间，低于25dB的声音听起来很吃力，超过100dB的声音会让人不适。AGC的作用就是将音量调整到人接受的范围。
AGC的调整分为模拟部分和数字部分，模拟部分是麦克风的采集增益，数字部分是音频数据的数字电平调整。


VAD
	Voice Activity Detection，语音端点监测，一般用于鉴别音频信号中的语音出现和语音消失



硬解码：
	由CPU核心GPU对高清视频进行解码，CPU占用率很低，画质效果比软解码略差，需要对播放器进行设置
	优点：播放流畅，低功耗
	缺点：受视频格式限制，功耗大，画质没有软解码好
软解码：
	由CPU负责解码进行播放
	优点：不受视频格式限制，画质略好于硬解码
	缺点：占用过高资源，对于高清视频可能没有硬解码流畅


SEI
	补充增强信息（Supplemental Enhancement Information），属于码流范畴，提供向视频码流中加入额外信息的方法，是H.264/H.265等视频压缩标准的特性之一
	特征：
		1、并非解码过程的必选项
		2、可能对解码过程（容错，纠错）有帮助
		3、集成在视频码流中
	信息：
		1、编码器参数
		2、视频版权信息
		3、摄像头参数
		4、内容生成过程中的剪辑事件（引发场景切换）




FFMPEG


命令：
error:use of undeclared identifier 'CODEC_FLAG_GLOBAL_HEADER'
改正：将CODEC_FLAG_GLOBAL_HEADER改为AV_CODEC_FLAG_GLOBAL_HEADER



推流命令：
ffmpeg -re -i /Users/lihuan/MyProjects/FFmpeg/Demo/PartyAnimal.mp4 -vcodec libx264 -acodec aac -f flv rtmp://localhost:1935/live/room



emcc与ffmpeg联合编译
emcc demo.c ~/em_ffmpeg/ffmpeg-3.4.8/dist/lib/libavformat.bc ~/em_ffmpeg/ffmpeg-3.4.8/dist/lib/libavcodec.bc ~/em_ffmpeg/ffmpeg-3.4.8/dist/lib/libavutil.bc ~/em_ffmpeg/ffmpeg-3.4.8/dist/lib/libswscale.bc ~/em_ffmpeg/ffmpeg-3.4.8/dist/lib/libswresample.bc ~/em_ffmpeg/ffmpeg-3.4.8/dist/lib/libavformat.bc -I ~/em_ffmpeg/ffmpeg-3.4.8/dist/include -s WASM=1 -s EXTRA_EXPORTED_RUNTIME_METHODS='["ccall", "cwrap"]' -s ALLOW_MEMORY_GROWTH=1 -s TOTAL_MEMORY=671088640 -o demo.js


ffplay播放yuv数据
	ffplay -ixxx.yuv -pix_fmt yuv420p -s 640x480


ffplay播放pcm
	ffplay -ar 16000 -channels 1 -f s16le -i xxx.pcm

Ffmpeg aac转mp3
	ffmpeg -i 002.aac -acodec libmp3lame 2.mp3

FFMPEG中的结构体
AVFormatContext
	描述一个多媒体文件的构成及其基本信息，存放视频编解码过程中的大部分信息。是其他输入、输出相关信息的容器。
	作为输入容器时，其成员变量struct AVInputFormat *iformat不能为空，其中包含了输入文件的音视频流信息，程序从输入容器中读出音视频包进行解码处理
	作为输出容器时，其成员变量struct AVOutputFormat *oformat不能为空，程序把编码好的音视频包写入到输出容器中
	成员变量：AVIOContext *pb I/O上下文，通过对该变量赋值可以改变输入源或输出目的
	成员变量：unsigned int nb_streams 音视频流数量
	成员变量：AVStream **streams 音视频流
	该结构体通常由avformat_open_input分配存储空间，avformat_input_close关闭

AVCodecContext
	描述编解码器上下文，包含众多编解码器需要的参数信息

AVCodec
	存储编解码器信息

AVPacket
	存放解码之前的数据。
	其data成员指向实际的缓冲区，在解码过程中可用av_read_frame填充缓冲区，av_free_packet释放缓冲区

AVFrame
	存放从AVPacket中解码出来的原始数据。
	使用av_frame_alloc来为其分配空间，但不会为AVPacket中的缓冲区分配空间

SwsCotext
	用于视频图像的转换


FFMPEG函数
av_register_all
	注册组件，调用此函数后，才能使用复用器、编码器等（新版本FFmpeg该函数为非必要函数）
	初始化libavformat和注册所有的muxers、demuxers、protocols
	其中会调用avcodec_register_all注册多种音视频格式的编解码器，注册各种文件的编解复用器

avformat_open_input
	打开多媒体数据，主要是探测码流的格式

avformat_find_stream_info
	读取部分音视频数据并获得相关信息，内部实现了解码器的查找，解码器的打开，音视频帧的读取、解码等

avcodec_find_decoder
	获取解码器

avcodec_open2
	初始化一个音视频解码器的AVCodecContext

av_read_frame
	读取码流中的音频若干帧或视频一帧

avcodec_decode_video2
	解码一帧视频数据，输入一个压缩编码的结构体AVPacket，输出一个解码后的结构体AVFrame
	若一个packet无法解码一个完整的视频帧，则在ffpmeg后台维护的缓存队列会持续等待多个packet，直到能够解码出一个完整的视频帧

sws_getContext
	初始化一个SwsContext，将传入的源图像，目标图像的宽高，像素格式，以及标志位分别复制给该SwsContext相应的字段

avpicture_fill
	为已经分配的空间的结构体AVPicture挂上一段用于保存数据的空间

sws_scale
	转换像素



int av_read_frame(AVFormatContext *s, AVPackt *pkt)
	返回值：
		成功返回0，失败返回值小于0或EOF，且pkt为空
	返回流中的下一帧数据。该函数返回文件中存储的东西，且不会校验是否是给解码器的有效帧。每次函数调用都会把文件中存储的数据分成一帧。它不会忽略有效帧中的无效数据，所以可能每次都把待解码的最大数据传递给解码器
	成功，返回带引用计数且有效校验的packet。返回的packet必须用av_packet_unref函数进行释放。对于视频数据来说，每个packet仅仅包含一帧视频信息。对于音频数据来说，除了一帧信息外，若这一帧有具体确切的大小的话也会包含一个表示帧大小的整数（如PCM或ADPCM格式）；若无具体尺寸大小，则仅仅包含一帧数据



int64_t av_rescale_rnd(int64_t a, int64_t b, int64_t c, enum AVRounding rnd)
	计算a * b / c的值且分5种方式取整
	FFmpeg中，将以时钟基表示的数值a转换成以时钟基b表示

int64_t av_rescale_q_rnd(int64_t a, AVRational bq, AVRational cq,enum AVRounding rnd)
	将以时钟基为c的时间戳a转换为以b为时钟基且以rnd方法取整

int64_t av_rescale_q(int64_t a, AVRational bq, AVRational cq)
	将以bHz时钟基表示的PTS/DTS值a转换成以cHz时钟基表示



推流步骤：

	注册组件（av_register_all）
	初始化网络库（avformat_network_init）
	打开输入流（avformat_open_input）
	获取流信息（avformat_find_stream_info）
	遍历输入格式结构体中的流的解码类型，若解码类型与需求类型一致则为输入流序号赋值退出
	为输出格式（推出的流）分配内存（avformat_alloc_output_context2）
	遍历输入流
	使用输入流中的解码器来为输出流初始化（avformat_new_stream）
	将输入流中的编码器信息复制到输出流中（avcodec_copy_context）
	设置标志位
	为从URL中获取到的资源创建并初始化一个AVIOContext结构体（avio_open）
	分配输出流的私有数据并将流的头部信息写入多媒体文件中（avformat_write_header）
	读取输入流中的frame到packet中（av_read_frame）
	校验packet中的pts是否等于AV_NOPTS_VALUE（若相等，则重新计算time_base，duration，pts，dts）
	校验packet中的流序号是否与输入流序号一致（若一致则表明是输入流，获取时间基并设置延时）
	将输入流时间基转换为输出流时间基（av_rescale_q_rnd）
	将packet中的内容写入输出流中（av_interleaved_write_frame）
	释放packet，将输出流写入多媒体文件中并释放私有数据（av_packet_unref、av_write_trailer）



拉流步骤（拉流并读取数据存入文件）：
	注册组件、初始化网络库（av_register_all、avformat_network_init）
	打开输入流、获取流信息（avformat_open_input、avformat_find_stream_info）
	遍历查找流中解码类型，若与需求一致则为输入流序号赋值并退出
	为输出格式分配一个AVFormatContext结构体（avformat_alloc_output_context2
	遍历输入流，使用输入流中的编码器为输出流初始化（avformat_new_stream）
	将输入流中的编码器信息复制给输出流（avformat_copy_context）
	设置标志位
	为从输出流的URL中获取的资源创建并初始化一个AVIOContext结构体（avio_open）
	为输出流分配私有数据并将流的头部信息写入多媒体文件中（avformat_write_header）
	读取输出流中的frame存入packet中（av_read_frame）
	将输入流时间基转换为输出流时间基（av_rescale_q_rnd）
	将packet中的数据写入输出流（av_interleaved_write_frame）
	释放packet和输出流（av_free_packet、av_write_trailer）



视频解码步骤：
	注册组件，初始化网络库（av_register_all，avformat_network_init）
	打开输入流，获取流信息（avformat_open_input，avformat_find_stream_info）
	遍历查找流中解码类型，若与需求一致则为输入流序号赋值并退出
	将输入流上下文结构体中的codec赋值给解码上下文
	查找解码器（avcodec_find_decoder）
	根据上面得到的codec为codecctx初始化（avcodec_open2）
	分配AVFrame空间和输出缓冲（av_frame_alloc）
	根据参数指定的图像和给定的数组设置data和linesize（av_image_fill_arrays）
	分配SwsContext的空间（sws_getContext）
	读取数据并解码（av_read_frame，avcodec_decodec_video2）
	判断是否有数据可以被解压
	若有，则进行图片存储格式的转换（sws_scale）
	数据读取结束后，刷新仍存在codec中的数据



音频解码步骤：
	注册组件，初始化网络库（av_register_all，avformat_network_init）
	打开输入流，获取流信息（avformat_open_input，avformat_find_stream_info）
	遍历查找流中解码类型，若与需求一致则为输入流序号赋值并退出
	将输入流上下文结构体中的codec赋值给解码上下文
	查找解码器（avcodec_find_decoder）
	根据上面得到的codec为codecctx初始化（avcodec_open2）
	分配AVFrame空间和输出缓冲（av_frame_alloc）
	设置和获取音频数据的各种参数（av_get_channel_layout_nb_channels，av_samples_get_buffer_size，av_get_default_channel_layout）
	分配SwsContext的空间并设置默认参数（swr_alloc，swr_alloc_set_opts，swr_init）
	读取数据并解码（av_read_frame，avcodec_decodec_vaudio4）
	判断是否有数据可以被解压
	若有，则进行图片存储格式的转换（swr_convert）



视频播放步骤：
	SDL初始化（SDL_Init）
	创建窗口（SDL_CreateWindow）
	创建渲染器（SDL_CreateRenderer）
	创建纹理（SDL_CreateTexture）
	创建刷新界面线程（SDL_CreateThread）
	循环阻塞等待事件产生（SDL_WaitEvent）
	刷新纹理（SDL_UpdateTexture）
	清空、复制渲染器，显示渲染数据



视频播放器：
	ffmpeg和SDL库的初始化—>创建拉流解析数据线程（拉流、解复用、读取、解析数据，并设置音频回调函数、视频处理线程。若为音频数据，则将音频包放入音频队列；视频则放入视频队列）—>视频/音频解码线程开始队视频/音频包队列中的数据包进行解码，解码后放入视频/音频帧队列中（由于音视频同步是将视频同步到音频中，还需要将视频数据同步）



SDL

子系统：
	Video（图像）：图像控制以及线程和事件管理
	Audio（声音）：声音控制
	JoyStick（摇杆）：游戏摇杆控制
	CD_ROM（光盘驱动器）：光盘媒体控制
	Window Management（视窗管理）：与视窗程序设计集成
	Event（事件驱动）：处理事件驱动


SDL播放视频流程：
初始化：
	1、初始化SDL——SDL_Init()
	2、创建窗口——SDL_CreateWindow
	3、基于窗口创建渲染器——SDL_CreateRenderer
	4、创建纹理——SDL_CreateTexture
循环显示画面：
	1、设置纹理数据——SDL_UpdateTexture
	2、纹理复制给渲染目标——SDL_RenderCopy
	3、显示——SDL_RenderPresent


函数：
	1、初始化SDL
		int SDLCALL SDL_Init(Uint32 flags)
		flags取值如下：
			SDL_INIT_TIMER：定时器
			SDL_INIT_AUDIO：音频
			SDL_INIT_VIDEO：视频
			SDL_INIT_HOYSTICK：摇杆
			SDL_INIT_HAPTIC：触摸屏
			SDL_INIT_GAMECONTROLLER：游戏控制器
			SDL_INIT_EVENTS：事件
			SDL_INIT_NOPARACHUTE：不捕获关键信号
			SDL_INIT_EVERYTHINS：包含以上所有选项

	2、创建窗口
		SDL_Window* SDLCALL SDL_CreateWindow(const char *title, int x, int y, int w, int h, Uint32 flags)
		参数：
			title：窗口标题
			x：窗口x坐标。也可设置为SDL_WINDOWPOS_CENTERED/SDL_WINDOWPOS_UNDEFINED
			y：窗口y坐标。同上
			w：窗口宽
			h：窗口高
			flags：SDL_WINDOW_FULLSCREEN	SDL_WINDOW_OPENGL	SDL_WINDOW_HIDDEN
					SDL_WINDOWBORDERLESS	SDL_WINDOW_RESIZABLE	SDL_WINDOW_MAXIMIZED
					SDL_WINDOW_MINIMIZED	SDL_WINDOW_INPUT_GRABBED	SDL_WINDOW_ALLOW_HIGHDPI

	3、基于窗口创建渲染器
		SDL_Renderer* SDLCALL SDL_CreateRenderer(SDL_WINDOW *window, int index, Uint32 flags)
		参数：
			window：渲染的目标窗口
			index：打算初始化的渲染设备的索引。-1则初始化默认的渲染设备
			flags：SDL_RENDERER_SOFTWARE：使用软件渲染
					SDL_RENDERER_ACCELERATED：使用硬件加速
					SDL_RENDERER_PRESENTVSYNC：和显示器的刷新率同步
					SDL_RENDERER_TARGETTEXTURE

	4、创建纹理
		SDL_Texture* SDLCALL SDL_CreateTexture(SDL_Renderer *renderer, Uint32 format, int access, int w, int h)
		参数：
			renderer：目标渲染器
			format：纹理格式
			access：SDL_TEXTUREACCESS_STATIC：变化极少
					SDL_TEXTUREACCESS_STREAMING：变化频繁
					SDL_TERXTUREACCESS_TARGET
			w：宽
			h：高

	5、设置纹理数据
		int SDLCALL SDL_UpdateTexture(SDL_Texture *texture, const SDL_Rect *rect, const void *pixels, int pitch);
		参数：
			texture：目标纹理
			rect：更新像素的矩形区域。NULL表示更新整个区域
			pixels：像素数据
			pitch：一行像素数据的字节数

	6、纹理复制给渲染目标
		int SDLCALL SDL_RenderCopy(SDL_Renderer *renderer, SDL_Texture *texture, const SDL_Rect *srcrect, const SDL_Rect *dstrect);
		参数：
			renderer：渲染目标
			texture：输入纹理
			srcrect：源区域（输入）。NULL表示整个纹理
			dstrect：目标区域（输出）。NUL表示整个渲染目标

	7、显示
		void SDLCALL SDL_RenderPresent(SDL_Renderer *renderer);
		参数：
			renderer：渲染目标



WebRTC

A和B之间要建立RTCPeerConnection连接流程：
     1. A创建一个RTCPeerConnection对象。

     2. A使用RTCPeerConnection .createOffer()方法产生一个offer（一个SDP会话描述）。

     3. A用生成的offer调用setLocalDescription()，设置成自己的本地会话描述。

     4. A将offer通过信令机制发送给B。

     5. B用A的offer调用setRemoteDescription()，设置成自己的远端会话描述，以便他的RTCPeerConnection知道A的设置。

     6. B调用createAnswer()生成answer

     7. B通过调用setLocalDescription()将其answer设置为本地会话描述。

     8. B然后使用信令机制将他的answer发回给A。

     9. A使用setRemoteDescription()将B的应答设置为远端会话描述。

     10. B received remote stream，此时，接收端已经可以播放视频。接着，触发 B 的 onaddstream 监听事件。获得远端的 video stream，注意此时 B 的 SDP 协商还未完成。

      11. 本地的 A candidate 的状态已经改变，触发 A onicecandidate。开始通过 B的addIceCandidate 方法将 A 添加进去。

      12. B setLocalDescription complete // B 的 SDP

      13. A setRemoteDescription complete // B 的 SDP

      14. A addIceCandidate success。A 添加成功

      15. 触发 oniceconnectionstatechange 检查 A 远端 candidate 的状态。当为 completed 状态时，则会触发 B onicecandidate 事件。

      16. B addIceCandidate success。


1. 呼叫者通过 navigator.mediaDevices.getUserMedia() 捕捉本地媒体。
2. 呼叫者创建一个RTCPeerConnection 并调用 RTCPeerConnection.addTrack() (注： addStream 已经过时。)
3. 呼叫者调用 ("RTCPeerConnection.createOffer()")来创建一个提议(offer).
4. 呼叫者调用 ("RTCPeerConnection.setLocalDescription()") 将提议(Offer)   设置为本地描述 (即，连接的本地描述).
5. setLocalDescription()之后, 呼叫者请求 STUN 服务创建ice候选(ice candidates)
6. 呼叫者通过信令服务器将提议(offer)传递至 本次呼叫的预期的接受者.
7. 接受者收到了提议(offer) 并调用 ("RTCPeerConnection.setRemoteDescription()") 将其记录为远程描述 (也就是连接的另一端的描述).
8. 接受者做一些可能需要的步骤结束本次呼叫：捕获本地媒体，然后通过RTCPeerConnection.addTrack()添加到连接中。
9. 接受者通过("RTCPeerConnection.createAnswer()")创建一个应答。
10. 接受者调用 ("RTCPeerConnection.setLocalDescription()") 将应答(answer)   设置为本地描述. 此时，接受者已经获知连接双方的配置了.
11. 接受者通过信令服务器将应答传递到呼叫者.
12. 呼叫者接受到应答.
13. 呼叫者调用 ("RTCPeerConnection.setRemoteDescription()") 将应答设定为远程描述. 如此，呼叫者已经获知连接双方的配置了.



STUN服务器是用来取外网地址的。
TURN服务器是在P2P失败时进行转发的


由于两个端有能力提出新的格式，而且格式可能不兼容，所以在重新协商期间，offer可能会被拒绝，但实际不会切换到另一个端，直到被其他端接收为止

当前描述（当前本地会话描述和当前远端会话描述）表示连接实际使用的描述，是双方已经完全同意使用的最新连接

待定描述（本地待定描述和远端待定描述），当读取描述时，若有带处理描述，则返回待定描述，否则返回当前描述

通过调用setLocalDescription或setRemoteDescription更改描述时，将制定的描述设置为待定描述，WebRTC层评估是否可以接受。当建议的描述达成一致时，当前描述的值将变为待定描述，待定描述的值变为null，表示没有待处理的描述


ICE候选地址
	除了交换关于媒体的信息(上面提到的Offer / Answer和SDP )中，对等体必须交换关于网络连接的信息。 这被称为ICE候选者，并详细说明了对等体能够直接或通过TURN服务器进行通信的可用方法。 通常，每个对点将优先提出最佳的ICE候选，逐次尝试到不佳的候选中。 理想情况下，候选地址是UDP(因为速度更快，媒体流能够相对容易地从中断恢复 )，但ICE标准也允许TCP候选。



信令过程
为了开启一个WebRTC会话，以下事件需要依次发生：
1. 每个Peer创建一个RTCPeerConnection对象，用来表示其WebRTC会话端。
2. 每个Peer建立一个icecandidate事件的响应程序，用来在监测到该事件时将这些candidates通过信令通道发送给另一个Peer。
3. 每个Peer建立一个track事件的响应程序，这个事件会在远程Peer添加一个track到其stream上时被触发。这个响应程序应将tracks和其消受者联系起来，例如<video>元素。
4. 呼叫者创建并与接收者共享一个唯一的标识符或某种令牌，使得它们之间的呼叫可以由信令服务器上的代码来识别。此标识符的确切内容和形式取决于您。
5. 每个Peer连接到一个约定的信令服务器，如WebSocket服务器，他们都知道如何与之交换消息。
6. 每个Peer告知信令服务器他们想加入同一WebRTC会话（由步骤4中建立的令牌标识）。
7. 描述，候选地址等 -- 之后会有更多


ICE重连有两个级别：全ICE重连会导致会话中的所有媒体流重新协商。 部分ICE重连允许ICE重新协商特定媒体流，而不是所有媒体流进行重新协商。

MediaStreamTrack
	表示流中的单个媒体轨道
	https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamTrack



MediaStream表示媒体内容流，它由音频和视频的轨道（ MediaStreamTrack ）组成。您可以通过调用MediaStream.getTracks()返回MediaStream的所有轨道，该方法将返回MediaStreamTrack对象的数组。


MediaStreamTrack
MediaStreamTrack具有audio或video的kind属性，指示其代表的媒体类型。可以通过切换每个轨道的enabled属性来使其静音。轨道具有布尔值remote ，它指示该轨道是否来自RTCPeerConnection并来自远程对等方。



var promise = navigator.mediaDevices.getDisplayMedia(constraints);
	提示用户选择和允许程序捕获显示内容或作为MedieStream的其部分内容
	参数：MediaStreamConstraints对象
	返回值：一个解析为包含视频轨道的MediaStream的Promise对象，内容来自用户选择的屏幕区域或者一个音频轨道

var mediaStreamTracks[] = mediaStream.getVideoTracks();
	返回表示流中视频轨道的MediaStreamTrack对象序列。视频轨道的kind属性为video



Var stream = MediaStream.clone();
	创建一个MediaStream的副本。新的对象有一个新的唯一的ID，也具有原MediaStream的MediaStreamTrack



var enumeratorPromise = navigator.mediaDevices.enumerateDevices();
	返回一个 Promise 。当完成时，它接收一个 MediaDeviceInfo 对象的数组。每个对象描述一个可用的媒体输入输出设备。



HTMLMediaElement.setSinkId(sinkId).then(function() { ... })
	为音频输出设备设置ID，返回一个Promise。当程序认证使用指定设备时才起作用



SDP中在audio以下a=fmtp:111 minptime=20;
	后面添加参数修改音频数据
	maxplaybackrate		采样率
	maxaveragebitrate	最大传输码率



Audio/Video Constraints中有的参数可以直接通过修改参数进行更改，否则需要修改SDP




过度约束



Opus
	支持6-510kbps的位速率
	支持恒定或可变位速率
	支持8-48kHz的取样率
	支持语音和音乐
	支持单声道和立体声
	支持2.5～60毫秒的帧大小
	支持浮点或定点算法



猜想
	x-google-min-bitrate等参数可限制码率，但可能会与b=AS行参数发生冲突
	or
	原有代码写入x-google-min-bitrate参数时写错行
	现象
		x-google-min-bitrate存在，b=AS不存在，码率限制可生效
		所有行都写入x-google-min-bitrate参数可生效
￼



音频引擎工作流程
	发送端采集音频信号，进行回声抑制、噪声消除、自动增益控制等前处理
	对处理后的数据进行编码，封装成数据包
	数据包网络传输
	接收端收包，解包，进行netEQ中的抖动消除、丢包补偿、解码，噪声抑制、自动增益控制等候处理，并将处理后的信号回传到发送端以进行回声抑制
	播放处理后的音频数据


NetEQ主要作用：
	消除由于网络环境变化造成的丢包和数据包到达间隔抖动

	数据包i的IAT + 数据包i在缓冲区的缓冲时间 == 数据包（i + 1）的IAT + 数据包（i + 1）在缓冲区的缓冲时间		（IAT为数据包到达时间间隔）


抖动
	接收端数据包i到达间隔与平均数据包到达间隔之差定义为抖动：
		𝐽𝑖  = 𝐸(𝑇) − 𝑇𝑖 	𝑖=1,2,...,𝑛
		T为第i个数据包到达间隔；E(T)为平均数据包到达间隔，当数据流为固定码率时E(T)应等于或接近于数据包发送间隔

	抖动通常使用抖动缓冲进行消除，待播放时再以平滑的速率从缓冲区中取出，经解压后播放。

	抖动缓冲控制算法：静态抖动缓冲控制、自适应抖动缓冲控制算法
		自适应抖动缓冲控制算法：缓冲区大小随着实际网络状况而变化，接收端将目前收到的数据包延迟和算法中保存的延迟信息比较，从而调整当前缓冲区大小


丢包隐藏
	又叫丢包补偿（Packet Loss Compensation， PLC），指试图产生一个相似的音频数据包或噪声包以替代丢失的数据包，基于音频的短时相似性，在丢包率低时（小于15%）尽可能提升音质

￼

	基于插入：插入一个填充包来修复丢包，填充包一般很简单（如静音包、噪声包或重复前面的包）。优势：实现简单；劣势：恢复效果差，没有利用其他语音信息重建信号
	基于重构：通过丢包前后的解码信息重构一个数据包，重构修补技术使用压缩算法来获得编码参数。优势：合成的丢包效果最好；劣势：计算量最大
	基于插值：通过模式匹配和插值技术创建和丢失包相似的数据包。考虑了音频的变化信息，比插入效果好，实现难度大


NetEQ模块：
	微处理单元（Micro Control Unit， MCU），根据当前情况做出相应动作，负责控制从网络收到的语音包在jitter  buffer里的插入和提取，同时控制DSP模块用哪种算法处理解码后的PCM数据
	数字信号处理（Digital Signal Process， DSP），将MCU中提取到的数据包进行信号处理，包括解码、加速、减速、丢包补偿、融合等


NetEQ处理过程
	1、将RTP语音包插入packet buffer
		1）在收到第一个RTP语音包后初始化netEQ
		2）解析RTP语音包，将其插入到packet buffer中（根据收到包的顺序依次插入）
		3）计算网络延迟optBufLevel
	2、从packet buffer中提取语音包解码和PCM信号处理
		1）将DSP模块的endTimeStamp赋值给playedOutTS，和sampleLeft（语音缓冲区中未播放的样本数）一同传给MCU，告知MCU当前DSP模块的播放情况
		2）看是否需要从packet buffer中取出语音包，以及是否能取出语音包。取包时遍历整个packet buffer方法，根据playedOutTS找到最小的大于等于playedOutTS的时间戳，记为availableTS，将其取出
		3）计算抖动缓冲延时bufLevelFilt
		4）根据网络延时和抖动缓冲延时以及上一帧的处理方式等因素决定本次的MCU控制命令
		5）若有从packet buffer中取到包就解码，否则不解码
		6）根据MCU给的控制命令对解码后和语音缓冲区中的数据做信号处理



向NetEQ中送入数据包
	1、InsertPacket，初始化NetEQ（如刷新缓冲区，重置timestamp_scaler，更新编解码器等），更新RTCP统计信息，解析RTP数据包，插入到packet buffer
	2、TimestampScaler类进行时间戳缩放。在解析网络的RTP包时，进行时间戳的转换。主要解决RTP采用的外部时间戳和代码内使用的内部时间戳不一致的情况
		外部时间戳：即RTP包携带的时间戳，表示RTO包文发送的时钟频率，单位为样本数（非时间单位）
			1）在语音中，外部时间戳通常等于PCM音频的采样率，RTP携带Opus编码数据包时，时钟频率固定为48kHz，但采样率可以有很多值
			2）视频中，无论何种编码格式，外部时间戳（时钟频率）均为90kHz
		内部时间戳：WebRTC使用的时间戳

		外部时间戳转内部时间戳即将外部时间戳按采样率缩放。假设初始内部时间戳为0，则：
			内部时间戳 += （外部时间戳间隔 * 采样率） / 外部时钟频率  或
			内部时间戳 += （外部时间戳间隔 / 外部时钟频率） * 采样率

		内部时间戳转外部时间戳即按照采样率放大。假设初始外部时间戳为0，则：
			外部时间戳+= （内部时间戳 * 外部时钟频率） / 采样率



从NetEQ中取处理后的音频数据
	主要是获取MCU决策，解码，VAD监测，DSP信号处理，将处理完成的音频数据存入语音缓冲区SuncBuffer，更新背噪参数，更新已播放时间戳等
	主要流程：
		1、获得下一步的操作operation
		2、根据operation提取数据到解码缓冲区（decoder_buffer_）
		3、处理后的数据存入算法缓冲区（algorithm_buffer_）
		4、将算法缓冲区的数据复制到语音缓冲区（sync_buffer_）
		5、将语音缓冲区中的数据提取到output

NetEQ的4个缓冲区
	抖动缓冲区：暂存从网络获得的音频数据包（packet buffer）
	解码缓冲区：抖动缓冲区中的数据包通过解码器解码后成为PCM原始音频数据，暂存到解码缓冲区（带符号的16位整形数组，固定长度5760？）（decoder_buffer）
	算法缓冲区：将解码缓冲区中的数据进行拉伸、平滑处理后将结果暂存到DSP算法缓冲区
	语音缓冲区：算法缓冲区中的数据会被放到语音缓冲区中，声卡每隔10ms会从语音缓冲区中提取长度为10ms的语音数据播放




WebRTC前向纠错编码（FEC）
	用于在网络丢包时恢复原始数据包，减少重传次数，减少延时，改善视频质量

Redundant Coding
	RED指冗余编码，且根据RFC 2198定义了一种RTP负载格式，用于携带冗余编码的音视频数据。RED封装的数据作为payload跟在RTP固定头（Fixed Header）后面。RED封装的数据有自己的Header。在一个RTP包中，会包含多个RED Header，指定了后面携带的编码数据信息
	Red Packet，即Redundant Coding产生的包。格式如下：
	0 1 2 3
	0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
	+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
	|F| block PT | timestamp offset | block length |
	+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
	F：1bit，表示后面跟着其他的Block Header，0表示这个Block Header是最后一个
	block PT：7bits。该block payload的payload type
	Timestamp Offset：14bits，无符号数，为RTP的Timestamp到这个Data Block的时间偏移
	Block Length：10bits，data block长度


